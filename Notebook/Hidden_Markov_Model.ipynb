{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hidden_Markov_Model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL9LiiLvNFYQ"
      },
      "source": [
        "## Importing Libraries\n",
        "Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXgZOxmG4nVK"
      },
      "source": [
        "# Importing libraries required for handling the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Importing libraries required for importing the data\n",
        "from google.colab import files\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9WXamgINRVN"
      },
      "source": [
        "Uploading the Data file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MsWqqV-WEeDt",
        "outputId": "1fa31942-514c-484c-c9d4-6f9869d6b1ec"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e0164fa-d92b-43ec-a868-f64dbdf69b53\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e0164fa-d92b-43ec-a868-f64dbdf69b53\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Shakespeare_data.csv to Shakespeare_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3vNbn8yNOKu"
      },
      "source": [
        "##Pre-Processing the dataset\n",
        "\n",
        "Cleaning the dataset, Dropping rows with NA values and resetting index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bfEAsg8reBgo",
        "outputId": "db3639c3-b5b4-493e-d7a3-c5ec6c35c58c"
      },
      "source": [
        "path = io.BytesIO(uploaded['Shakespeare_data.csv'])\n",
        "df = pd.read_csv(path)\n",
        "df = df.dropna()\n",
        "df = df.reset_index()\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Dataline</th>\n",
              "      <th>Play</th>\n",
              "      <th>PlayerLinenumber</th>\n",
              "      <th>ActSceneLine</th>\n",
              "      <th>Player</th>\n",
              "      <th>PlayerLine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>Henry IV</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1.1</td>\n",
              "      <td>KING HENRY IV</td>\n",
              "      <td>So shaken as we are, so wan with care,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Henry IV</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1.2</td>\n",
              "      <td>KING HENRY IV</td>\n",
              "      <td>Find we a time for frighted peace to pant,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>Henry IV</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1.3</td>\n",
              "      <td>KING HENRY IV</td>\n",
              "      <td>And breathe short-winded accents of new broils</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>Henry IV</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1.4</td>\n",
              "      <td>KING HENRY IV</td>\n",
              "      <td>To be commenced in strands afar remote.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Henry IV</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1.5</td>\n",
              "      <td>KING HENRY IV</td>\n",
              "      <td>No more the thirsty entrance of this soil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105147</th>\n",
              "      <td>111390</td>\n",
              "      <td>111391</td>\n",
              "      <td>A Winters Tale</td>\n",
              "      <td>38.0</td>\n",
              "      <td>5.3.179</td>\n",
              "      <td>LEONTES</td>\n",
              "      <td>Is troth-plight to your daughter. Good Paulina,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105148</th>\n",
              "      <td>111391</td>\n",
              "      <td>111392</td>\n",
              "      <td>A Winters Tale</td>\n",
              "      <td>38.0</td>\n",
              "      <td>5.3.180</td>\n",
              "      <td>LEONTES</td>\n",
              "      <td>Lead us from hence, where we may leisurely</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105149</th>\n",
              "      <td>111392</td>\n",
              "      <td>111393</td>\n",
              "      <td>A Winters Tale</td>\n",
              "      <td>38.0</td>\n",
              "      <td>5.3.181</td>\n",
              "      <td>LEONTES</td>\n",
              "      <td>Each one demand an answer to his part</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105150</th>\n",
              "      <td>111393</td>\n",
              "      <td>111394</td>\n",
              "      <td>A Winters Tale</td>\n",
              "      <td>38.0</td>\n",
              "      <td>5.3.182</td>\n",
              "      <td>LEONTES</td>\n",
              "      <td>Perform'd in this wide gap of time since first</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105151</th>\n",
              "      <td>111394</td>\n",
              "      <td>111395</td>\n",
              "      <td>A Winters Tale</td>\n",
              "      <td>38.0</td>\n",
              "      <td>5.3.183</td>\n",
              "      <td>LEONTES</td>\n",
              "      <td>We were dissever'd: hastily lead away.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>105152 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         index  ...                                       PlayerLine\n",
              "0            3  ...           So shaken as we are, so wan with care,\n",
              "1            4  ...       Find we a time for frighted peace to pant,\n",
              "2            5  ...   And breathe short-winded accents of new broils\n",
              "3            6  ...          To be commenced in strands afar remote.\n",
              "4            7  ...        No more the thirsty entrance of this soil\n",
              "...        ...  ...                                              ...\n",
              "105147  111390  ...  Is troth-plight to your daughter. Good Paulina,\n",
              "105148  111391  ...       Lead us from hence, where we may leisurely\n",
              "105149  111392  ...            Each one demand an answer to his part\n",
              "105150  111393  ...   Perform'd in this wide gap of time since first\n",
              "105151  111394  ...           We were dissever'd: hastily lead away.\n",
              "\n",
              "[105152 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7rJ6ijVNdx5"
      },
      "source": [
        "Focussing on one play as limited resources available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvG2SIoWgZ0n",
        "outputId": "2028d91e-662a-41f0-ae3d-6c898b64048e"
      },
      "source": [
        "df_onePlay = df.loc[df['Play']=='Henry IV']\n",
        "print(df_onePlay.shape)\n",
        "text_corpus = df_onePlay['PlayerLine']\n",
        "text_corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3044, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0               So shaken as we are, so wan with care,\n",
              "1           Find we a time for frighted peace to pant,\n",
              "2       And breathe short-winded accents of new broils\n",
              "3              To be commenced in strands afar remote.\n",
              "4            No more the thirsty entrance of this soil\n",
              "                             ...                      \n",
              "3039    To fight with Glendower and the Earl of March.\n",
              "3040       Rebellion in this land shall lose his sway,\n",
              "3041           Meeting the cheque of such another day:\n",
              "3042          And since this business so fair is done,\n",
              "3043         Let us not leave till all our own be won.\n",
              "Name: PlayerLine, Length: 3044, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gBRB9hUNjNX"
      },
      "source": [
        "## Defining Hidden Markov Model\n",
        "\n",
        "HMM Parameters:\n",
        "\n",
        "*   Transition Matrix (T) -  This is the joint probability matrix that tells us the probability of transition from one hidden state to the other state.\n",
        "*   Emission Matrix (E) - This is the probability distribution for the observations given the hidden states.\n",
        "*   Initial Probabilities (pi) - These are the initial probabilities for all the hidden states.\n",
        "\n",
        "Used forward backward algorithm, specifically Baum Welch algorithm to compute the optimal Transition matrix and Emission Matrix.\n",
        "\n",
        "Reference: https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86\n",
        "\n",
        "All of the matrix and initial probabilities are random in the first iteration, then using Baum Welch algorithm for forward backward Expectation Maximization, there are 3 steps as follows:\n",
        "\n",
        "For expectation step there are 2 phases:\n",
        "* Forward phase: with the observations in this step we propogate the information forward, this computes the alpha function values.\n",
        "* Backward Phase: The backward phase enables us to propogate the information (error) backwards and correct the model itsel. This computes the beta function values.\n",
        "\n",
        "The Maximization step has one phase:\n",
        "* Update Phase: The update phase takes the alpha and the beta values we calculated in the expectation steps and caluclates the gamma and xi related to them. With the gamma and xi we maximize the HMM parameters, i.e. Transition matrix (T), Emission Matrix (E), and the initial probabilities for the hidden states (pi).\n",
        "\n",
        "The train funciton just call these phases function in order for all the observations, and converges when the difference between the old copy and the updated copy crosses a threshold or maximum iterations are reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdH--3u3TZHA"
      },
      "source": [
        "class HMM:\n",
        "  def __init__(self,n_hidden_states, obs_states,init_probs = None):\n",
        "    self.hidden_states = [i for i in range(n_hidden_states)]\n",
        "    self.n_hidden_states = n_hidden_states\n",
        "    self.obs_states = obs_states\n",
        "    self.n_obs_states = len(obs_states)\n",
        "    # Hidden variable probability transition stochastic matrix\n",
        "    self.T = np.random.rand(len(self.hidden_states),len(self.hidden_states))\n",
        "    # Observation given hidden probabilty emission matrix\n",
        "    self.E = np.random.rand(len(self.hidden_states),self.n_obs_states)\n",
        "    self.initial_probabilities(len(self.hidden_states))\n",
        "    for i in range(len(self.hidden_states)):\n",
        "      self.T[i,:] /= self.T[i,:].sum(axis=0) # Normalizing\n",
        "      self.E[i,:] /= self.E[i,:].sum(axis=0) # Normalizing\n",
        "    self.word_map = self.word_map(self.obs_states)\n",
        "    self.index_map = self.index_map()\n",
        "\n",
        "  def initial_probabilities(self,n_hidden_states):\n",
        "    size = n_hidden_states # Total number of states\n",
        "    randomUniformProbs = np.random.rand(size) / (size**2) + 1 / size # Random distribution between [0,1] for total number of states\n",
        "    randomUniformProbs /= randomUniformProbs.sum(axis=0) # Normalizing\n",
        "    self.pi = np.array(randomUniformProbs).reshape(1,-1)\n",
        "\n",
        "  def forwardPhase(self,observation):\n",
        "    # total alphas, one for every hidden state upto time k (k states)\n",
        "    alpha = np.zeros((len(self.hidden_states), len(observation)))\n",
        "    alpha[:,0] = self.pi[:] * self.E[:,observation[0]]\n",
        "\n",
        "    for k in range(len(observation) - 1):\n",
        "        for i in range(self.n_hidden_states):\n",
        "            sum = 0\n",
        "            for j in range(self.n_hidden_states):\n",
        "                sum += alpha[j,k] * self.T[j,i]\n",
        "            alpha[i,k+1] = self.E[i,observation[k+1]] * sum\n",
        "    \n",
        "    return alpha\n",
        "\n",
        "  def backwardPhase(self,observation):\n",
        "    # The beta function is defined as the conditional probability of the observed data \n",
        "    # from time k+1 given the state at time k\n",
        "    beta = np.zeros((len(self.hidden_states), len(observation)))\n",
        "    beta[:,-1] = 1\n",
        "\n",
        "    for k in range(len(observation) - 2, -1, -1):\n",
        "      for i in range(self.n_hidden_states):\n",
        "        for j in range(self.n_hidden_states):\n",
        "          beta[i,k] += beta[j,k+1] * self.T[i,j] * self.E[j,observation[k+1]]\n",
        "    return beta\n",
        "\n",
        "  def updatePhase(self, alpha, beta, observation):\n",
        "    gamma = np.zeros((len(self.hidden_states), len(observation)))\n",
        "\n",
        "    for k in range(len(observation)):\n",
        "      denom = 0\n",
        "      for i in range(len(self.hidden_states)):\n",
        "        gamma[i,k] = alpha[i,k] * beta[i,k]\n",
        "        denom += gamma[i,k]\n",
        "      gamma[:,k] = gamma[:,k] / denom\n",
        "\n",
        "    xi = np.zeros((len(self.hidden_states), len(self.hidden_states), len(observation)))\n",
        "    for k in range(len(observation)-1):\n",
        "      denom = 0\n",
        "      for i in range(self.n_hidden_states):\n",
        "        for j in range(self.n_hidden_states):\n",
        "          xi[i,j,k] = alpha[i,k] * self.T[i,j] * beta[j,k+1] * self.E[j,observation[k+1]]\n",
        "          denom += xi[i,j,k]\n",
        "      xi[:,:,k] = xi[:,:,k] / denom\n",
        "    return gamma, xi\n",
        "\n",
        "  def word_map(self, uniqueWords):\n",
        "    map = {}\n",
        "    i = 0\n",
        "    for word in uniqueWords:\n",
        "      if word not in map:\n",
        "        map[word] = i\n",
        "        i += 1\n",
        "    return map\n",
        "\n",
        "  def index_map(self):\n",
        "    map = {}\n",
        "    for word in self.word_map:\n",
        "      index = self.word_map[word]\n",
        "      map[index] = word\n",
        "    return map\n",
        "\n",
        "  def decodeText(self, index):\n",
        "    if index not in self.index_map:\n",
        "      return self.index_map[0]\n",
        "    return self.index_map[index]\n",
        "\n",
        "  def encodeText(self, text):\n",
        "    indexedWords = []\n",
        "    for seq in text:\n",
        "      seq_by_index = []\n",
        "      for word in seq:\n",
        "        seq_by_index.append(self.word_to_index(word))\n",
        "      indexedWords.append(seq_by_index)\n",
        "    return indexedWords\n",
        "\n",
        "  def word_to_index(self, word):\n",
        "    if word not in self.word_map: #for unknown words\n",
        "      return 0\n",
        "    return self.word_map[word]\n",
        "\n",
        "  def getUniques(self, observations):   \n",
        "    uniques = []\n",
        "    for sentence in observations:\n",
        "      words = re.split('\\s+', sentence)\n",
        "      for word in words:\n",
        "        if word not in uniques:\n",
        "          uniques.append(word)\n",
        "    return uniques\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"HMM \\n-> Hidden states: {} \\n-> observables: {}. \\npi = {}\\nT = {}\\nE = {}\".format(\n",
        "        self.hidden_states, self.obs_states, self.pi, self.T, self.E)\n",
        "    \n",
        "  def trainModel(self, textCorpus, maxIter=1000, threshold=0.0001):\n",
        "    uniqueWords = self.getUniques(textCorpus)\n",
        "    indexedWords = self.encodeText(uniqueWords)\n",
        "    for it in range(maxIter):\n",
        "      start = time.time()\n",
        "      print(\"Iteration {}\".format(it))\n",
        "      # save old stuff\n",
        "      old_T = np.copy(self.T)\n",
        "      old_E = np.copy(self.E)\n",
        "      old_pi = np.copy(self.pi)\n",
        "      gammas = []\n",
        "      xis = []\n",
        "\n",
        "      for seq, n in zip(indexedWords, range(len(indexedWords))):\n",
        "        if n % 50 == 0:\n",
        "          print(\"Line {}\".format(n))\n",
        "        alpha = self.forwardPhase(seq)\n",
        "        beta = self.backwardPhase(seq)\n",
        "        g,xi = self.updatePhase(alpha,beta,seq)\n",
        "        gammas.append(g)\n",
        "        xis.append(xi)\n",
        "\n",
        "      print(\"Expectation-step complete\")\n",
        "\n",
        "      print(\"maximizing initial probabilities (pi)\")\n",
        "      for i in range(len(self.hidden_states)):\n",
        "        temp_pi = 0\n",
        "        for T in range(len(textCorpus)):\n",
        "          temp_pi += gammas[T][i,0]\n",
        "        self.pi[0,i] = temp_pi\n",
        "      self.pi = self.pi / np.sum(self.pi)\n",
        "\n",
        "      # a\n",
        "      print(\"maxmizing Transition Probabilities (T)\")\n",
        "      for i in range(self.n_hidden_states):\n",
        "        denom = 0\n",
        "        for T in range(len(indexedWords)):\n",
        "          denom += np.sum(gammas[T][i,0:-2])\n",
        "\n",
        "        for j in range(self.n_hidden_states):\n",
        "          numer = 0\n",
        "          for T in range(len(indexedWords)):\n",
        "            numer += np.sum(xis[T][i,j,0:-2])\n",
        "          self.T[i,j] = numer / denom\n",
        "\n",
        "      # b\n",
        "      print(\"maximizing Emission Matrix (E)\")\n",
        "      self.E = np.zeros((self.n_hidden_states, self.n_obs_states))\n",
        "      for i in range(self.n_hidden_states):\n",
        "        denom = 0\n",
        "        for seq, T in zip(indexedWords, range(len(indexedWords))):\n",
        "          denom += np.sum(gammas[T][i,:])\n",
        "          for word, t in zip(seq, range(len(seq))):\n",
        "            self.E[i, word] += gammas[T][i,t]\n",
        "\n",
        "        self.E[i,:] = self.E[i,:] / denom\n",
        "\n",
        "      print(\"Maximization-step Complete\")\n",
        "      diffT = np.absolute(np.linalg.norm(self.T) - np.linalg.norm(old_T))\n",
        "      diffE = np.absolute(np.linalg.norm(self.E) - np.linalg.norm(old_E))\n",
        "      diffPi = np.absolute(np.linalg.norm(self.pi) - np.linalg.norm(old_pi))\n",
        "      total_diff = diffT + diffE + diffPi\n",
        "      print(\"Total diff: {}\".format(total_diff))\n",
        "      print(\"Total time: {}\".format(time.time() - start))\n",
        "      if total_diff <= threshold:\n",
        "        break\n",
        "\n",
        "    return self.T, self.E, self.pi\n",
        "\n",
        "  def genText(self, length):\n",
        "    state = np.random.choice(range(self.n_hidden_states), p=self.pi[0])\n",
        "    s = ''\n",
        "    for i in range(length):\n",
        "      s += self.decodeText(np.random.choice(list(self.E[state]), p=list(self.E[state]))) + ' ' # Emission in the state\n",
        "      state = np.random.choice(range(self.n_hidden_states), p=self.T[state]) # Transition from state to state\n",
        "\n",
        "    print(s)\n",
        "  \n",
        "  def viterbi(self,sequence,n):\n",
        "    # s = re.split('\\s+', sequence)\n",
        "    s = self.encodeText(sequence)\n",
        "\n",
        "    t1 = np.zeros((self.n_hidden_states, len(s)))\n",
        "    t2 = np.zeros((self.n_hidden_states, len(s)))\n",
        "\n",
        "    for i in range(self.n_hidden_states):\n",
        "      t1[i, 0] = self.pi[0,i] * self.E[i, s[0]]\n",
        "      t2[i, 0] = 0\n",
        "\n",
        "    for i in range(1, len(s)):\n",
        "      for j in range(self.n_hidden_states):\n",
        "        t = t1[:, i - 1] * self.T[:][j] * self.E[j, s[i]]\n",
        "        t1[j][i] = np.max(t)\n",
        "        t2[j][i] = np.argmax(t)\n",
        "\n",
        "    z = np.zeros(len(s))\n",
        "    x = np.zeros(len(s))\n",
        "\n",
        "    z[-1] = np.argmax(t1[:, -1])\n",
        "    x[-1] = int(z[-1])\n",
        "\n",
        "    for i in range(len(s) - 1, 0, -1):\n",
        "      z[i - 1] = t2[int(z[i]), i]\n",
        "      x[i - 1] = z[i - 1]\n",
        "\n",
        "    state = x[-1]\n",
        "\n",
        "    s = ''\n",
        "\n",
        "    for i in range(n):\n",
        "      state = np.random.choice(range(self.n_hidden_states), p=self.T[int(state)])\n",
        "      s += self.decodeText(np.random.choice(list(self.E[int(state)]), p=list(self.E[int(state)]))) + ' '\n",
        "\n",
        "    print(s)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkDIif9pNomL"
      },
      "source": [
        "HMM training and results\n",
        "\n",
        "Provided a limited text corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUqjHXQChBpU",
        "outputId": "cb5e5f2a-7d2e-4195-fa8e-e472bac79f9f"
      },
      "source": [
        "hmm = HMM(10,text_corpus[1:100])\n",
        "T,E,pi = hmm.trainModel(text_corpus[1:100])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "Line 0\n",
            "Line 50\n",
            "Line 100\n",
            "Line 150\n",
            "Line 200\n",
            "Line 250\n",
            "Line 300\n",
            "Line 350\n",
            "Line 400\n",
            "Line 450\n",
            "Expectation-step complete\n",
            "maximizing initial probabilities (pi)\n",
            "maxmizing Transition Probabilities (T)\n",
            "maximizing Emission Matrix (E)\n",
            "Maximization-step Complete\n",
            "Total diff: 3.0774593964091963\n",
            "Total time: 1.211127519607544\n",
            "Iteration 1\n",
            "Line 0\n",
            "Line 50\n",
            "Line 100\n",
            "Line 150\n",
            "Line 200\n",
            "Line 250\n",
            "Line 300\n",
            "Line 350\n",
            "Line 400\n",
            "Line 450\n",
            "Expectation-step complete\n",
            "maximizing initial probabilities (pi)\n",
            "maxmizing Transition Probabilities (T)\n",
            "maximizing Emission Matrix (E)\n",
            "Maximization-step Complete\n",
            "Total diff: 2.248201624865942e-14\n",
            "Total time: 1.1606712341308594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoQZ-L1VNqb7"
      },
      "source": [
        "Generating text using the trained HMM model using Baum Welch algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xuh9aPLhHoq",
        "outputId": "1adef110-53a0-4730-9ec9-c6075df4d660"
      },
      "source": [
        "hmm.genText(50)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, And breathe short-winded accents of new broils Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, Find we a time for frighted peace to pant, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy6ehtZ-NsHU"
      },
      "source": [
        "Using Viterbi algorithm to generate the best path given a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-buIiycoTNc",
        "outputId": "e3d6a176-5212-456a-8761-21244f914440"
      },
      "source": [
        "hmm.viterbi(\"to be \",1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Find we a time for frighted peace to pant, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ToX7tEpPqou"
      },
      "source": [
        "**Conclusion:** The Hidden Markov Model (HMM) performs well on these type of problems, from the results I can see that the model may perform good with better/more training. Currently with 10 hidden states and only 100 lines of observations from the whole text corpus the HMM model does not have much idea about how shakespeare writes. It was only able to learn one line from the given observations and hence performs bad currently. I am hoping with more training, and with more hidden states the results will improve for both generating random sequences, and for Viterbi (generating best path for given sequence).\n",
        "\n",
        "**Future work:** I considered playing around with the text corpus with frequencies of the words, with some threshold for frequency, if the words appear more than the threshold count it as a hidden state. I did start with the implementation for this but could not finish it as had some difficulties at the end while training (laptop crashed, few functionalities deleted)"
      ]
    }
  ]
}